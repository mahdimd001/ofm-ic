{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsize CLIP model use OSF\n",
    "\n",
    "In this tutorial, we will show you how to quickly extract various subnets from super-CLIP optimized by OSF. The subnets is more efficient with siginificantly reduced parameters and FLOPs, while maintaining the competitive performance as the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import dependency packages, and utility function for calculate model size.\n",
    "\n",
    "Also make sure you have GPU enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import functools\n",
    "from ofm import OFM\n",
    "from tqdm import tqdm\n",
    "\n",
    "import functools\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import Parameter\n",
    "def calculate_params(model):\n",
    "    \"\"\"calculate the number of parameters in the model\n",
    "    Args:\n",
    "        model: the model to be evaluated\n",
    "    Returns:\n",
    "        total_params: the number of parameters in the model\n",
    "        percentage: the percentage of trainable parameters in the model\n",
    "    \"\"\"\n",
    "\n",
    "    millions = 1000000\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, \"weight\") and isinstance(module.weight, Parameter):\n",
    "            total_params += torch.prod(torch.tensor(module.weight.size())).item()\n",
    "\n",
    "    return total_params / millions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 19:03:11 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    83W / 500W |  12265MiB / 81920MiB |     51%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    700079      C   python                           5290MiB |\n",
      "|    0   N/A  N/A    768885      C   python                           6972MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load super-CLIP model optimized by OSF use Huggingface `CLIPModel` API.\n",
    "\n",
    "You can find our published checkpoint at [README.md](README.md) \n",
    "\n",
    "The model checkpoint we used is [here](https://huggingface.co/yusx-swapp/ofm-clip-base-patch32-cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_name = \"yusx-swapp/ofm-clip-base-patch32-cifar10\" #You can find our published checkpoint at README.md\n",
    "model = CLIPModel.from_pretrained(ckpt_name)\n",
    "processor = CLIPProcessor.from_pretrained(ckpt_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the CLIP model to a supernet via OSF AIP :`OFM supernet class`, with only 1 line of code.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernet = OFM(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a subnet from the supernet, we can simply call the `resource_aware_model` API. There are multiple ways to get a subnet, such as specifying the target model structure, get smallest size model with a elastic space, or get a random downsized model. More details can be found in the `examples/post_training_deployment`.\n",
    "\n",
    "In this example, we randomly sample a subnet within the search space via `random_resource_aware_model` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model, param, arc_config = supernet.random_resource_aware_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the model size between the original model and the subnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model has 151.105536M parameters\n",
      "Downsized model has 112.832512M parameters\n",
      "Total model size reduction: 38.27302400000001M\n"
     ]
    }
   ],
   "source": [
    "original_model_params = calculate_params(model)\n",
    "ds_model_params = calculate_params(ds_model)\n",
    "print(f\"Original model has {original_model_params}M parameters\")\n",
    "print(f\"Downsized model has {ds_model_params}M parameters\")\n",
    "print(f\"Total model size reduction: {original_model_params - ds_model_params}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the subnet's performance on the CIFAR-10 dataset via the metric of **accuracy, F1, precision, and recall**.\n",
    "\n",
    "First, we load the CIFAR-10 dataset and preprocess it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cifar10\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label_to_text = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"This function is used to collate the data samples into batches.\n",
    "    It is used to supply the DataLoader with the collate_fn argument.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of samples from the dataset\n",
    "    returns:\n",
    "        A dictionary of tensors containing the batched samples\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "    }\n",
    "\n",
    "def transform_eval(example_batch, processor):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor(\n",
    "        text=[label_to_text[label] for label in range(10)],\n",
    "        images=example_batch[\"img\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    inputs[\"labels\"] = example_batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "prepared_test = dataset[\"test\"].with_transform(\n",
    "    functools.partial(transform_eval, processor=processor)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use use the `evaluate` function bellow to calculate the subnet's performance on the cifar-10 dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/40 [00:00<?, ?it/s]Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Evaluation:   2%|▎         | 1/40 [00:06<04:03,  6.24s/it, Accuracy=0.8000, F1 Score=0.8667, Precision=1.0000, Recall=0.8000]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:   8%|▊         | 3/40 [00:06<01:01,  1.66s/it, Accuracy=0.8000, F1 Score=0.7967, Precision=0.8625, Recall=0.8000]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:   8%|▊         | 3/40 [00:06<01:01,  1.66s/it, Accuracy=0.8000, F1 Score=0.7979, Precision=0.8783, Recall=0.8000]Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  12%|█▎        | 5/40 [00:06<00:29,  1.20it/s, Accuracy=0.7800, F1 Score=0.7765, Precision=0.8801, Recall=0.7800]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  12%|█▎        | 5/40 [00:06<00:29,  1.20it/s, Accuracy=0.8000, F1 Score=0.7911, Precision=0.8798, Recall=0.8000]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  18%|█▊        | 7/40 [00:06<00:16,  1.95it/s, Accuracy=0.8000, F1 Score=0.7909, Precision=0.8436, Recall=0.8000]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  18%|█▊        | 7/40 [00:06<00:16,  1.95it/s, Accuracy=0.8125, F1 Score=0.8021, Precision=0.8522, Recall=0.8125]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  32%|███▎      | 13/40 [00:07<00:06,  4.40it/s, Accuracy=0.7923, F1 Score=0.7757, Precision=0.8135, Recall=0.7923]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  45%|████▌     | 18/40 [00:08<00:03,  6.07it/s, Accuracy=0.8333, F1 Score=0.8218, Precision=0.8573, Recall=0.8333]Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  48%|████▊     | 19/40 [00:08<00:03,  6.41it/s, Accuracy=0.8316, F1 Score=0.8188, Precision=0.8580, Recall=0.8316]Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  52%|█████▎    | 21/40 [00:08<00:03,  6.17it/s, Accuracy=0.8333, F1 Score=0.8253, Precision=0.8688, Recall=0.8333]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  57%|█████▊    | 23/40 [00:08<00:02,  6.16it/s, Accuracy=0.8348, F1 Score=0.8249, Precision=0.8719, Recall=0.8348]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  60%|██████    | 24/40 [00:09<00:02,  6.62it/s, Accuracy=0.8333, F1 Score=0.8234, Precision=0.8702, Recall=0.8333]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  60%|██████    | 24/40 [00:09<00:02,  6.62it/s, Accuracy=0.8320, F1 Score=0.8236, Precision=0.8623, Recall=0.8320]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  65%|██████▌   | 26/40 [00:09<00:01,  8.15it/s, Accuracy=0.8346, F1 Score=0.8262, Precision=0.8655, Recall=0.8346]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  68%|██████▊   | 27/40 [00:09<00:01,  8.20it/s, Accuracy=0.8370, F1 Score=0.8289, Precision=0.8679, Recall=0.8370]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  70%|███████   | 28/40 [00:09<00:02,  4.61it/s, Accuracy=0.8393, F1 Score=0.8319, Precision=0.8724, Recall=0.8393]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  72%|███████▎  | 29/40 [00:10<00:02,  4.90it/s, Accuracy=0.8379, F1 Score=0.8327, Precision=0.8728, Recall=0.8379]Unused or unrecognized kwargs: padding.\n",
      "Evaluation:  75%|███████▌  | 30/40 [00:10<00:01,  5.35it/s, Accuracy=0.8387, F1 Score=0.8324, Precision=0.8693, Recall=0.8387]Unused or unrecognized kwargs: padding.\n",
      "Evaluation: 100%|██████████| 40/40 [00:11<00:00,  3.48it/s, Accuracy=0.8375, F1 Score=0.8294, Precision=0.8684, Recall=0.8375]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(eval_dataloader):\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "    )\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ds_model.to(device)\n",
    "    ds_model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    progress_bar = tqdm(eval_dataloader, desc=\"Evaluation\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        images = batch[\"pixel_values\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = ds_model(pixel_values=images, input_ids=input_ids)\n",
    "            logits = outputs.logits_per_image\n",
    "            predicted_labels = torch.argmax(logits, dim=1).to(\"cpu\").tolist()\n",
    "\n",
    "        true_labels.extend(labels.to(\"cpu\"))\n",
    "        pred_labels.extend(predicted_labels)\n",
    "\n",
    "        # Calculate intermediate metrics\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "        precision = precision_score(true_labels, pred_labels, average=\"weighted\")\n",
    "        recall = recall_score(true_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"Accuracy\": f\"{accuracy:.4f}\",\n",
    "                \"F1 Score\": f\"{f1:.4f}\",\n",
    "                \"Precision\": f\"{precision:.4f}\",\n",
    "                \"Recall\": f\"{recall:.4f}\",\n",
    "            }\n",
    "        )\n",
    "    eval_metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "    return eval_metrics\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "            prepared_test,\n",
    "            batch_size=256,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            # drop_last=True,\n",
    "        )\n",
    "eval_metrics = evaluate(eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we compare print out the subnet's performance on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8375, 'f1': 0.8293750643570631, 'precision': 0.8683762448655351, 'recall': 0.8375}\n"
     ]
    }
   ],
   "source": [
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
